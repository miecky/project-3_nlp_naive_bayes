{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 3 - Web APIs & Classification\n",
    "\n",
    "## Description:\n",
    "\n",
    "For project 3, the goal is two-fold:\n",
    "\n",
    "1. Using Reddit's API to collect posts from two subreddits.\n",
    "2. Using NLP to train a classifier on which subreddit a given post came from. This is a binary classification problem.\n",
    "\n",
    "## Project Structure:\n",
    "\n",
    "- Part 1 - Web APIs and Data Collection\n",
    "- Part 2 - EDA, Data Cleaning, and Feature Engineering\n",
    "- Part 3a - Modeling: Naive-Bayes\n",
    "- Part 3b - Modeling: Logistic-Regression\n",
    "- Part 4 - Visualization with Scattertext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1 - Web APIs and Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Table of Content\n",
    "\n",
    "- [1.0-Import Libraries](#1.0---Import-Libraries)\n",
    "- [1.1-Reddit API](#1.1---Reddit-API)\n",
    "- [1.2-Generate CSV File Names](#1.2---Generate-CSV-File-Names)\n",
    "- [1.3-Collecting Posts](#1.3---Collecting-Posts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 - Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import re\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 - Reddit API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of urls\n",
    "'''to add more urls, label each url using url_# scheme'''\n",
    "\n",
    "url_1 = 'http://www.reddit.com/r/breastcancer.json'\n",
    "url_2 = 'http://www.reddit.com/r/airquality.json'\n",
    "url_list = [url_1,\n",
    "            url_2\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 - Generate CSV File Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of csv file names\n",
    "def gen_filename(url_list):\n",
    "    pattern = r'r/(\\w+\\.)json'\n",
    "    return [re.findall(pattern, url)[0].lower() +'csv' for url in url_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breastcancer.csv', 'airquality.csv']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate filenames\n",
    "filenames = gen_filename(url_list)\n",
    "filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 - Collecting Posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for generating posts\n",
    "'''Function to collect posts from subreddits:\n",
    "   Inputs:\n",
    "          url_list: list, list of urls (reddit APIs)\n",
    "          post_n:   int, number of post for each url\n",
    "          filenames: list of csv file names\n",
    "   Output: csv files \n",
    "'''\n",
    "def post_collector(url_list, post_n, filenames):\n",
    "    headers = {'User-agent': 'Kai Bot 2.0'}\n",
    "    for n in range(len(url_list)):  # loop through reddit url lists\n",
    "        posts = []                  \n",
    "        after = None                # After stores the id for the last post on the page. \n",
    "        for i in range(int(post_n/25)): # Each page contain 25 posts. The code will loop through n/25 pages.\n",
    "            print(f'Downloaded posts {25*(i+1)} from {url_list[n]}')\n",
    "            if after == None:       \n",
    "                params = {}\n",
    "            else:\n",
    "                params = {'after': after} \n",
    "            res = requests.get(url_list[n], params=params, headers=headers) # load page\n",
    "            if res.status_code == 200:\n",
    "                the_json = res.json()  # save post data in Json structure\n",
    "                posts.extend(the_json['data']['children'])\n",
    "                after = the_json['data']['after'] # update 'after' to the ID of the last post of current page \n",
    "            else:\n",
    "                print(res.status_code)\n",
    "                break\n",
    "            time.sleep(5) # delay request for 5 seconds. \n",
    "        pd.DataFrame(posts).to_csv('./dataset/'+filenames[n], index=False) # save to csv file using input filenames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded posts 25 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 50 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 75 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 100 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 125 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 150 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 175 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 200 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 225 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 250 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 275 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 300 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 325 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 350 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 375 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 400 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 425 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 450 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 475 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 500 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 525 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 550 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 575 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 600 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 625 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 650 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 675 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 700 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 725 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 750 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 775 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 800 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 825 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 850 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 875 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 900 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 925 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 950 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 975 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 1000 from http://www.reddit.com/r/breastcancer.json\n",
      "Downloaded posts 25 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 50 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 75 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 100 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 125 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 150 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 175 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 200 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 225 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 250 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 275 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 300 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 325 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 350 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 375 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 400 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 425 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 450 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 475 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 500 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 525 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 550 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 575 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 600 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 625 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 650 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 675 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 700 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 725 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 750 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 775 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 800 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 825 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 850 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 875 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 900 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 925 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 950 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 975 from http://www.reddit.com/r/airquality.json\n",
      "Downloaded posts 1000 from http://www.reddit.com/r/airquality.json\n"
     ]
    }
   ],
   "source": [
    "# Collect posts and save to csv\n",
    "post_collector(url_list, 1000, filenames)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
